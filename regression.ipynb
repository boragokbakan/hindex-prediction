{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Public-GNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# h-Index Prediction with GCNs\n",
        "This notebook is the final submission for the X-INF554 Data Challenge at Ecole Polytechnique, where the aim was to predict authors' h-Index given:\n",
        "\n",
        "\n",
        "*   Co-authorship graph between authors\n",
        "*   List of at most 5 abstracts for each author.\n",
        "\n",
        "The authors are represented as feature nodes where the features are:\n",
        "\n",
        "*   aggregate abstract vectors (i.e. mean BERT embedding of authors' abstracts, doc2vec etc.)\n",
        "*   Graph based features\n",
        "\n",
        "Our approach achieved an MSE of **45.29** on the private leaderboard, putting us at the 5th place out of 63 teams. For reference, the first team achieved 42.66 and the mean score was at 81.80 (std 35.33).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JRRffI4vrFPB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa7AzREMidN_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Cm71xK_h3t_z",
        "outputId": "49f7e59d-f371-4f3b-e1cb-00286044aa3b"
      },
      "source": [
        "author_papers = pd.read_csv(\"author_papers.txt.zip\", delimiter=':', names=['author', 'paper'])\n",
        "author_papers['paper'] = author_papers['paper'].apply(lambda x: x.split('-'))\n",
        "#author_papers['n_papers'] = author_papers['paper'].apply(len)\n",
        "author_papers = author_papers.explode('paper')\n",
        "author_papers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>1510273386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>1827736641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>1588673897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>2252711322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>2123653597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217800</th>\n",
              "      <td>2908499439</td>\n",
              "      <td>2081432213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217800</th>\n",
              "      <td>2908499439</td>\n",
              "      <td>2070621672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217800</th>\n",
              "      <td>2908499439</td>\n",
              "      <td>2079679191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217800</th>\n",
              "      <td>2908499439</td>\n",
              "      <td>32110345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217800</th>\n",
              "      <td>2908499439</td>\n",
              "      <td>2013975658</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>965741 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            author       paper\n",
              "0          1036332  1510273386\n",
              "0          1036332  1827736641\n",
              "0          1036332  1588673897\n",
              "0          1036332  2252711322\n",
              "0          1036332  2123653597\n",
              "...            ...         ...\n",
              "217800  2908499439  2081432213\n",
              "217800  2908499439  2070621672\n",
              "217800  2908499439  2079679191\n",
              "217800  2908499439    32110345\n",
              "217800  2908499439  2013975658\n",
              "\n",
              "[965741 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7UOkY2oowukX",
        "outputId": "46c88ec4-5b40-4919-efa8-4abffa65612e"
      },
      "source": [
        "train = pd.read_pickle(\"train_author_agg_vectors.pkl\").drop(['max_embedding', 'first_embedding'], axis=1).rename(columns={'mean_embedding': 'mean_sbert'})\n",
        "test = pd.read_pickle(\"test_author_agg_vectors.pkl\").drop(['max_embedding', 'first_embedding'], axis=1).rename(columns={'mean_embedding': 'mean_sbert'})\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>hindex</th>\n",
              "      <th>n_papers</th>\n",
              "      <th>mean_sbert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1964267543</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.052135132, 0.022680586, 0.041294187, -0.05...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2153592714</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.02547187, -0.02848028, -0.014876259, -0.04...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>217158525</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.005974621, 0.006903825, -0.017989326, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2123103677</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[-0.033690836, 0.03599136, -0.052308355, 0.044...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2067710487</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0011541683, -0.09693919, -0.035319697, 0.01...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174236</th>\n",
              "      <td>2225897966</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.010317355, 0.07318564, -0.0676425, -0.0559...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174237</th>\n",
              "      <td>2612161910</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.065617844, 0.10500442, -0.09921767, -0.027...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174238</th>\n",
              "      <td>2575614996</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.03478216, -0.022265393, 0.0045300666, 0.04...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174239</th>\n",
              "      <td>2078153944</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0136339385, -0.05201685, -0.0034561865, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174240</th>\n",
              "      <td>1992421474</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[0.034862068, 0.014916212, 0.022052275, -0.000...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174241 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            author  ...                                         mean_sbert\n",
              "0       1964267543  ...  [-0.052135132, 0.022680586, 0.041294187, -0.05...\n",
              "1       2153592714  ...  [-0.02547187, -0.02848028, -0.014876259, -0.04...\n",
              "2        217158525  ...  [-0.005974621, 0.006903825, -0.017989326, -0.0...\n",
              "3       2123103677  ...  [-0.033690836, 0.03599136, -0.052308355, 0.044...\n",
              "4       2067710487  ...  [0.0011541683, -0.09693919, -0.035319697, 0.01...\n",
              "...            ...  ...                                                ...\n",
              "174236  2225897966  ...  [-0.010317355, 0.07318564, -0.0676425, -0.0559...\n",
              "174237  2612161910  ...  [-0.065617844, 0.10500442, -0.09921767, -0.027...\n",
              "174238  2575614996  ...  [-0.03478216, -0.022265393, 0.0045300666, 0.04...\n",
              "174239  2078153944  ...  [0.0136339385, -0.05201685, -0.0034561865, 0.0...\n",
              "174240  1992421474  ...  [0.034862068, 0.014916212, 0.022052275, -0.000...\n",
              "\n",
              "[174241 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "EJJ48K0_a2yB",
        "outputId": "6f0d144f-8a15-4624-a7e8-a549635abcf8"
      },
      "source": [
        "train[\"extra\"] = pd.Series(list(np.load(\"train_graph_n2v_d2v.npy\")))\n",
        "test[\"extra\"] = pd.Series(list(np.load(\"test_graph_n2v_d2v.npy\")))\n",
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>n_papers</th>\n",
              "      <th>mean_sbert</th>\n",
              "      <th>extra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>915630815</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.07671193, 0.06058638, 0.017426852, -0.0379...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 10.0, 10.0, 0.0, 1.0, 1.698462...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1236455448</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.011177505, 0.008547784, 0.0030871811, -0.0...</td>\n",
              "      <td>[5.0, 5.0, 1.0, 4.2, 4.4, 1.0, 3.0, 5.84279909...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2694593333</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[0.006413276, -0.009554359, -0.043468893, -0.0...</td>\n",
              "      <td>[16.0, 16.0, 1.0, 10.75, 11.5625, 1.0, 7.0, 1....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2137926699</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[-0.0077553736, 0.039409973, -0.07448004, 0.01...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 26.0, 27.0, 0.0, 1.0, 1.182820...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2883694285</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[-0.09723279, -0.020298773, -0.0020541456, 0.0...</td>\n",
              "      <td>[3.0, 4.0, 1.0, 10.0, 8.75, 1.0, 3.0, 2.742900...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43555</th>\n",
              "      <td>2145559725</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[-0.01877177, -0.031387135, -0.06260405, 0.023...</td>\n",
              "      <td>[2.0, 2.0, 0.0, 13.0, 13.5, 0.0, 2.0, 1.636933...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43556</th>\n",
              "      <td>2168342616</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.0007464085, -0.04247803, 0.028256837, 0.00...</td>\n",
              "      <td>[3.0, 8.0, 2.0, 11.0, 14.25, 3.5, 3.0, 2.57296...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43557</th>\n",
              "      <td>2162797290</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.05684532, -0.026625771, 0.016216638, 0.060...</td>\n",
              "      <td>[6.0, 9.0, 1.0, 91.66666666666667, 96.22222222...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43558</th>\n",
              "      <td>294576894</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.06492782, -0.03364247, -0.0504133, -0.0294...</td>\n",
              "      <td>[7.0, 7.0, 0.0, 4.571428571428571, 5.0, 0.0, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43559</th>\n",
              "      <td>2508925065</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.011323395, -0.02003629, -0.0155694, -0.042...</td>\n",
              "      <td>[18.0, 18.0, 3.0, 30.77777777777778, 68.888888...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>43560 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           author  ...                                              extra\n",
              "0       915630815  ...  [1.0, 1.0, 0.0, 10.0, 10.0, 0.0, 1.0, 1.698462...\n",
              "1      1236455448  ...  [5.0, 5.0, 1.0, 4.2, 4.4, 1.0, 3.0, 5.84279909...\n",
              "2      2694593333  ...  [16.0, 16.0, 1.0, 10.75, 11.5625, 1.0, 7.0, 1....\n",
              "3      2137926699  ...  [1.0, 1.0, 0.0, 26.0, 27.0, 0.0, 1.0, 1.182820...\n",
              "4      2883694285  ...  [3.0, 4.0, 1.0, 10.0, 8.75, 1.0, 3.0, 2.742900...\n",
              "...           ...  ...                                                ...\n",
              "43555  2145559725  ...  [2.0, 2.0, 0.0, 13.0, 13.5, 0.0, 2.0, 1.636933...\n",
              "43556  2168342616  ...  [3.0, 8.0, 2.0, 11.0, 14.25, 3.5, 3.0, 2.57296...\n",
              "43557  2162797290  ...  [6.0, 9.0, 1.0, 91.66666666666667, 96.22222222...\n",
              "43558   294576894  ...  [7.0, 7.0, 0.0, 4.571428571428571, 5.0, 0.0, 4...\n",
              "43559  2508925065  ...  [18.0, 18.0, 3.0, 30.77777777777778, 68.888888...\n",
              "\n",
              "[43560 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "wOXISNQRwB6-",
        "outputId": "0b25e810-5bbf-474f-f467-b5302dd6c20b"
      },
      "source": [
        "train['extra_bert'] = pd.Series(list(np.load(\"embedding_all-MiniLM-L12-v2_mean_train.npy\")))\n",
        "test['extra_bert'] = pd.Series(list(np.load(\"embedding_all-MiniLM-L12-v2_mean_test.npy\")))\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>hindex</th>\n",
              "      <th>n_papers</th>\n",
              "      <th>mean_sbert</th>\n",
              "      <th>extra</th>\n",
              "      <th>extra_bert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1964267543</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.052135132, 0.022680586, 0.041294187, -0.05...</td>\n",
              "      <td>[5.0, 6.0, 1.0, 22.8, 22.666666666666668, 1.0,...</td>\n",
              "      <td>[-0.05630162, 0.09752295, -0.1141102, -0.13589...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2153592714</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.02547187, -0.02848028, -0.014876259, -0.04...</td>\n",
              "      <td>[2.0, 6.0, 2.0, 10.0, 18.333333333333332, 2.5,...</td>\n",
              "      <td>[0.04320468, 0.14857285, -0.16441649, -0.21340...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>217158525</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.005974621, 0.006903825, -0.017989326, -0.0...</td>\n",
              "      <td>[2.0, 3.0, 0.0, 3.5, 7.0, 0.0, 2.0, 3.42108641...</td>\n",
              "      <td>[0.00582764, 0.09294338, -0.15769197, -0.24381...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2123103677</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[-0.033690836, 0.03599136, -0.052308355, 0.044...</td>\n",
              "      <td>[7.0, 7.0, 1.0, 15.714285714285714, 16.4285714...</td>\n",
              "      <td>[0.01378435, 0.07472001, -0.04285565, -0.10068...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2067710487</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0011541683, -0.09693919, -0.035319697, 0.01...</td>\n",
              "      <td>[2.0, 2.0, 0.0, 3.5, 3.5, 0.0, 1.0, 4.25552092...</td>\n",
              "      <td>[-0.01707689, 0.04883542, -0.02549759, -0.0859...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174236</th>\n",
              "      <td>2225897966</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.010317355, 0.07318564, -0.0676425, -0.0559...</td>\n",
              "      <td>[3.0, 3.0, 0.0, 12.666666666666666, 12.6666666...</td>\n",
              "      <td>[0.01870065, 0.03190672, -0.01903249, -0.02287...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174237</th>\n",
              "      <td>2612161910</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.065617844, 0.10500442, -0.09921767, -0.027...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 6.0, 6.0, 0.0, 1.0, 1.68192524...</td>\n",
              "      <td>[0.00334274, 0.04985746, -0.04555392, -0.03742...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174238</th>\n",
              "      <td>2575614996</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.03478216, -0.022265393, 0.0045300666, 0.04...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 13.0, 13.0, 0.0, 1.0, 1.284886...</td>\n",
              "      <td>[-0.00639988, 0.00703147, -0.01361149, -0.0672...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174239</th>\n",
              "      <td>2078153944</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0136339385, -0.05201685, -0.0034561865, 0.0...</td>\n",
              "      <td>[4.0, 4.0, 0.0, 6.75, 7.0, 0.0, 4.0, 4.3931628...</td>\n",
              "      <td>[0.02069795, 0.01978348, -0.03274036, -0.11306...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174240</th>\n",
              "      <td>1992421474</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[0.034862068, 0.014916212, 0.022052275, -0.000...</td>\n",
              "      <td>[5.0, 5.0, 0.0, 7.6, 7.8, 0.0, 4.0, 4.38694768...</td>\n",
              "      <td>[0.01676006, 0.00762678, -0.13381339, -0.13810...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174241 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            author  ...                                         extra_bert\n",
              "0       1964267543  ...  [-0.05630162, 0.09752295, -0.1141102, -0.13589...\n",
              "1       2153592714  ...  [0.04320468, 0.14857285, -0.16441649, -0.21340...\n",
              "2        217158525  ...  [0.00582764, 0.09294338, -0.15769197, -0.24381...\n",
              "3       2123103677  ...  [0.01378435, 0.07472001, -0.04285565, -0.10068...\n",
              "4       2067710487  ...  [-0.01707689, 0.04883542, -0.02549759, -0.0859...\n",
              "...            ...  ...                                                ...\n",
              "174236  2225897966  ...  [0.01870065, 0.03190672, -0.01903249, -0.02287...\n",
              "174237  2612161910  ...  [0.00334274, 0.04985746, -0.04555392, -0.03742...\n",
              "174238  2575614996  ...  [-0.00639988, 0.00703147, -0.01361149, -0.0672...\n",
              "174239  2078153944  ...  [0.02069795, 0.01978348, -0.03274036, -0.11306...\n",
              "174240  1992421474  ...  [0.01676006, 0.00762678, -0.13381339, -0.13810...\n",
              "\n",
              "[174241 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxi20GHf3pKu"
      },
      "source": [
        "graph = nx.read_edgelist('coauthorship.edgelist', delimiter=' ', nodetype=int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqRB2LOU3rFH",
        "outputId": "700671ba-685e-4d08-c6e5-cd3131cdf611"
      },
      "source": [
        "graph.number_of_edges(), graph.number_of_nodes(), "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1718164, 217801)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8vcjEi14xrQ",
        "outputId": "af3b9e58-6f17-43a6-a166-bb3193bdc27e"
      },
      "source": [
        "from tqdm import tqdm\n",
        "core_number = nx.core_number(graph) # dict with node_number\n",
        "n_neighbours = {node: graph.degree(node) for node in tqdm(core_number.keys())}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 217801/217801 [00:00<00:00, 331454.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "7XSlRYbYu3Lx",
        "outputId": "0f8720d5-898b-4fde-f39f-b3109c94b386"
      },
      "source": [
        "train['neighbours'] = train.author.apply(lambda x : list(graph.neighbors(x)))\n",
        "test['neighbours'] = test.author.apply(lambda x : list(graph.neighbors(x)))\n",
        "\n",
        "mapdict = train[['author', 'hindex']].set_index('author').to_dict()['hindex']\n",
        "\n",
        "def neighbours_to_index(neighbours):\n",
        "\n",
        "  nums = []\n",
        "  for n in neighbours:\n",
        "    if n in mapdict.keys():\n",
        "      nums.append(mapdict[n])\n",
        "\n",
        "  if len(nums) > 0:\n",
        "    return sum(nums) / len(nums)\n",
        "\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\n",
        "train['mean_hindex_neighbours'] = train.neighbours.apply(neighbours_to_index)\n",
        "test['mean_hindex_neighbours'] = test.neighbours.apply(neighbours_to_index)\n",
        "\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>hindex</th>\n",
              "      <th>n_papers</th>\n",
              "      <th>mean_sbert</th>\n",
              "      <th>extra</th>\n",
              "      <th>extra_bert</th>\n",
              "      <th>neighbours</th>\n",
              "      <th>mean_hindex_neighbours</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1964267543</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.052135132, 0.022680586, 0.041294187, -0.05...</td>\n",
              "      <td>[5.0, 6.0, 1.0, 22.8, 22.666666666666668, 1.0,...</td>\n",
              "      <td>[-0.05630162, 0.09752295, -0.1141102, -0.13589...</td>\n",
              "      <td>[307593211, 2383048336, 1643434777, 834507041,...</td>\n",
              "      <td>21.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2153592714</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.02547187, -0.02848028, -0.014876259, -0.04...</td>\n",
              "      <td>[2.0, 6.0, 2.0, 10.0, 18.333333333333332, 2.5,...</td>\n",
              "      <td>[0.04320468, 0.14857285, -0.16441649, -0.21340...</td>\n",
              "      <td>[1233913860, 2124461921]</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>217158525</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.005974621, 0.006903825, -0.017989326, -0.0...</td>\n",
              "      <td>[2.0, 3.0, 0.0, 3.5, 7.0, 0.0, 2.0, 3.42108641...</td>\n",
              "      <td>[0.00582764, 0.09294338, -0.15769197, -0.24381...</td>\n",
              "      <td>[2502610808, 2261647917]</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2123103677</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[-0.033690836, 0.03599136, -0.052308355, 0.044...</td>\n",
              "      <td>[7.0, 7.0, 1.0, 15.714285714285714, 16.4285714...</td>\n",
              "      <td>[0.01378435, 0.07472001, -0.04285565, -0.10068...</td>\n",
              "      <td>[2064546257, 2147281624, 2284716930, 201270321...</td>\n",
              "      <td>11.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2067710487</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0011541683, -0.09693919, -0.035319697, 0.01...</td>\n",
              "      <td>[2.0, 2.0, 0.0, 3.5, 3.5, 0.0, 1.0, 4.25552092...</td>\n",
              "      <td>[-0.01707689, 0.04883542, -0.02549759, -0.0859...</td>\n",
              "      <td>[2168344074, 2043762735]</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174236</th>\n",
              "      <td>2225897966</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.010317355, 0.07318564, -0.0676425, -0.0559...</td>\n",
              "      <td>[3.0, 3.0, 0.0, 12.666666666666666, 12.6666666...</td>\n",
              "      <td>[0.01870065, 0.03190672, -0.01903249, -0.02287...</td>\n",
              "      <td>[2110438662, 2191889675, 2071929315]</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174237</th>\n",
              "      <td>2612161910</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.065617844, 0.10500442, -0.09921767, -0.027...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 6.0, 6.0, 0.0, 1.0, 1.68192524...</td>\n",
              "      <td>[0.00334274, 0.04985746, -0.04555392, -0.03742...</td>\n",
              "      <td>[271172287]</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174238</th>\n",
              "      <td>2575614996</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.03478216, -0.022265393, 0.0045300666, 0.04...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 13.0, 13.0, 0.0, 1.0, 1.284886...</td>\n",
              "      <td>[-0.00639988, 0.00703147, -0.01361149, -0.0672...</td>\n",
              "      <td>[2222643512]</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174239</th>\n",
              "      <td>2078153944</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0136339385, -0.05201685, -0.0034561865, 0.0...</td>\n",
              "      <td>[4.0, 4.0, 0.0, 6.75, 7.0, 0.0, 4.0, 4.3931628...</td>\n",
              "      <td>[0.02069795, 0.01978348, -0.03274036, -0.11306...</td>\n",
              "      <td>[2597672571, 2306030925, 1270584344, 1991427769]</td>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174240</th>\n",
              "      <td>1992421474</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[0.034862068, 0.014916212, 0.022052275, -0.000...</td>\n",
              "      <td>[5.0, 5.0, 0.0, 7.6, 7.8, 0.0, 4.0, 4.38694768...</td>\n",
              "      <td>[0.01676006, 0.00762678, -0.13381339, -0.13810...</td>\n",
              "      <td>[2223178354, 2481487314, 1855590767, 260114106...</td>\n",
              "      <td>5.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174241 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            author  ...  mean_hindex_neighbours\n",
              "0       1964267543  ...               21.800000\n",
              "1       2153592714  ...               20.000000\n",
              "2        217158525  ...                2.000000\n",
              "3       2123103677  ...               11.833333\n",
              "4       2067710487  ...                     NaN\n",
              "...            ...  ...                     ...\n",
              "174236  2225897966  ...               10.000000\n",
              "174237  2612161910  ...                8.000000\n",
              "174238  2575614996  ...                     NaN\n",
              "174239  2078153944  ...                5.500000\n",
              "174240  1992421474  ...                5.666667\n",
              "\n",
              "[174241 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtf3eusYdlNA"
      },
      "source": [
        "fill_mean_hindex = train['mean_hindex_neighbours'].mean()\n",
        "\n",
        "train.loc[train.mean_hindex_neighbours.isna(), 'mean_hindex_neighbours'] = train.loc[train.mean_hindex_neighbours.isna(), 'mean_hindex_neighbours'].apply(lambda x: fill_mean_hindex)\n",
        "test.loc[test.mean_hindex_neighbours.isna(), 'mean_hindex_neighbours'] = test.loc[test.mean_hindex_neighbours.isna(), 'mean_hindex_neighbours'].apply(lambda x: fill_mean_hindex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58BOHCYHx9Oy"
      },
      "source": [
        "noise = train['mean_hindex_neighbours'].std() * .5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9tJoOnL9DT"
      },
      "source": [
        "train['core_number'] = train['author'].map(core_number)\n",
        "train['n_neighbours'] = train['author'].map(n_neighbours)\n",
        "\n",
        "test['core_number'] = test['author'].map(core_number)\n",
        "test['n_neighbours'] = test['author'].map(n_neighbours)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNTfpwmF8F68"
      },
      "source": [
        "from networkx.linalg.graphmatrix import adjacency_matrix\n",
        "\n",
        "A = adjacency_matrix(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU2DWOFWDjb_"
      },
      "source": [
        "nodes = list(graph.nodes)\n",
        "name2idx = {nodes[i]: i for i in range(len(nodes))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "YxU8kvx3LV6K",
        "outputId": "643015d2-2e78-4825-e325-6985faf2189f"
      },
      "source": [
        "train['nodeidx'] = train.author.map(name2idx)\n",
        "test['nodeidx'] = test.author.map(name2idx)\n",
        "\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>hindex</th>\n",
              "      <th>n_papers</th>\n",
              "      <th>mean_sbert</th>\n",
              "      <th>extra</th>\n",
              "      <th>extra_bert</th>\n",
              "      <th>neighbours</th>\n",
              "      <th>mean_hindex_neighbours</th>\n",
              "      <th>core_number</th>\n",
              "      <th>n_neighbours</th>\n",
              "      <th>nodeidx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1964267543</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.052135132, 0.022680586, 0.041294187, -0.05...</td>\n",
              "      <td>[5.0, 6.0, 1.0, 22.8, 22.666666666666668, 1.0,...</td>\n",
              "      <td>[-0.05630162, 0.09752295, -0.1141102, -0.13589...</td>\n",
              "      <td>[307593211, 2383048336, 1643434777, 834507041,...</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>43275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2153592714</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.02547187, -0.02848028, -0.014876259, -0.04...</td>\n",
              "      <td>[2.0, 6.0, 2.0, 10.0, 18.333333333333332, 2.5,...</td>\n",
              "      <td>[0.04320468, 0.14857285, -0.16441649, -0.21340...</td>\n",
              "      <td>[1233913860, 2124461921]</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>132559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>217158525</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[-0.005974621, 0.006903825, -0.017989326, -0.0...</td>\n",
              "      <td>[2.0, 3.0, 0.0, 3.5, 7.0, 0.0, 2.0, 3.42108641...</td>\n",
              "      <td>[0.00582764, 0.09294338, -0.15769197, -0.24381...</td>\n",
              "      <td>[2502610808, 2261647917]</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>165398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2123103677</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[-0.033690836, 0.03599136, -0.052308355, 0.044...</td>\n",
              "      <td>[7.0, 7.0, 1.0, 15.714285714285714, 16.4285714...</td>\n",
              "      <td>[0.01378435, 0.07472001, -0.04285565, -0.10068...</td>\n",
              "      <td>[2064546257, 2147281624, 2284716930, 201270321...</td>\n",
              "      <td>11.833333</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>89808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2067710487</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0011541683, -0.09693919, -0.035319697, 0.01...</td>\n",
              "      <td>[2.0, 2.0, 0.0, 3.5, 3.5, 0.0, 1.0, 4.25552092...</td>\n",
              "      <td>[-0.01707689, 0.04883542, -0.02549759, -0.0859...</td>\n",
              "      <td>[2168344074, 2043762735]</td>\n",
              "      <td>16.135587</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>209408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174236</th>\n",
              "      <td>2225897966</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.010317355, 0.07318564, -0.0676425, -0.0559...</td>\n",
              "      <td>[3.0, 3.0, 0.0, 12.666666666666666, 12.6666666...</td>\n",
              "      <td>[0.01870065, 0.03190672, -0.01903249, -0.02287...</td>\n",
              "      <td>[2110438662, 2191889675, 2071929315]</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>130257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174237</th>\n",
              "      <td>2612161910</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.065617844, 0.10500442, -0.09921767, -0.027...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 6.0, 6.0, 0.0, 1.0, 1.68192524...</td>\n",
              "      <td>[0.00334274, 0.04985746, -0.04555392, -0.03742...</td>\n",
              "      <td>[271172287]</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>216172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174238</th>\n",
              "      <td>2575614996</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[-0.03478216, -0.022265393, 0.0045300666, 0.04...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 13.0, 13.0, 0.0, 1.0, 1.284886...</td>\n",
              "      <td>[-0.00639988, 0.00703147, -0.01361149, -0.0672...</td>\n",
              "      <td>[2222643512]</td>\n",
              "      <td>16.135587</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>65960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174239</th>\n",
              "      <td>2078153944</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0136339385, -0.05201685, -0.0034561865, 0.0...</td>\n",
              "      <td>[4.0, 4.0, 0.0, 6.75, 7.0, 0.0, 4.0, 4.3931628...</td>\n",
              "      <td>[0.02069795, 0.01978348, -0.03274036, -0.11306...</td>\n",
              "      <td>[2597672571, 2306030925, 1270584344, 1991427769]</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>43677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174240</th>\n",
              "      <td>1992421474</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[0.034862068, 0.014916212, 0.022052275, -0.000...</td>\n",
              "      <td>[5.0, 5.0, 0.0, 7.6, 7.8, 0.0, 4.0, 4.38694768...</td>\n",
              "      <td>[0.01676006, 0.00762678, -0.13381339, -0.13810...</td>\n",
              "      <td>[2223178354, 2481487314, 1855590767, 260114106...</td>\n",
              "      <td>5.666667</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>111167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174241 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            author  hindex  n_papers  ... core_number n_neighbours nodeidx\n",
              "0       1964267543     4.0       5.0  ...           5            5   43275\n",
              "1       2153592714    13.0       5.0  ...           2            2  132559\n",
              "2        217158525     8.0       5.0  ...           2            2  165398\n",
              "3       2123103677    11.0       3.0  ...           6            7   89808\n",
              "4       2067710487     3.0       2.0  ...           1            2  209408\n",
              "...            ...     ...       ...  ...         ...          ...     ...\n",
              "174236  2225897966     1.0       1.0  ...           3            3  130257\n",
              "174237  2612161910     1.0       1.0  ...           1            1  216172\n",
              "174238  2575614996     1.0       1.0  ...           1            1   65960\n",
              "174239  2078153944     1.0       2.0  ...           4            4   43677\n",
              "174240  1992421474     1.0       4.0  ...           4            5  111167\n",
              "\n",
              "[174241 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruu585KEMRr7"
      },
      "source": [
        "train = train.set_index('nodeidx').sort_index()\n",
        "test = test.set_index('nodeidx').sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOPL5WNbRYPP"
      },
      "source": [
        "na_train = train.isna().any(axis=1)\n",
        "na_test = test.isna().any(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj_TZzc1wel6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOrBjOvtSvJv"
      },
      "source": [
        "# Fill N/A values with mean\n",
        "fill_n_papers = train[~na_train].n_papers.mean() * 0\n",
        "fill_mean_sbert = train[~na_train].mean_sbert.mean() * 0\n",
        "#fill_mean_glove = train[~na_train].mean_glove.mean()\n",
        "\n",
        "\n",
        "train.loc[na_train, \"mean_sbert\"] = train.loc[na_train, \"mean_sbert\"].apply(lambda x: fill_mean_sbert)\n",
        "#train.loc[na_train, \"mean_glove\"] = train.loc[na_train, \"mean_glove\"].apply(lambda x: fill_mean_glove)\n",
        "train.loc[na_train, \"n_papers\"] = train.loc[na_train, \"n_papers\"].apply(lambda x: fill_n_papers)\n",
        "\n",
        "\n",
        "test.loc[na_test, \"mean_sbert\"] = test.loc[na_test, \"mean_sbert\"].apply(lambda x: fill_mean_sbert)\n",
        "#test.loc[na_test, \"mean_glove\"] = test.loc[na_test, \"mean_glove\"].apply(lambda x: fill_mean_glove)\n",
        "test.loc[na_test, \"n_papers\"] = test.loc[na_test, \"n_papers\"].apply(lambda x: fill_n_papers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWpC3GC2UcOH"
      },
      "source": [
        "feature_cols = ['n_papers', 'core_number', 'n_neighbours',]\n",
        "\n",
        "train_idx = train.index.values\n",
        "train_idx, val1_idx = train_test_split(train_idx, test_size=.3, )\n",
        "test_idx = test.index.values\n",
        "\n",
        "X = pd.concat([train,test]).sort_index()\n",
        "y = X.hindex.values#[:, None]\n",
        "\n",
        "\n",
        "X = np.hstack((np.vstack(X.mean_sbert.values), \n",
        "               np.vstack(X.extra.values), \n",
        "               np.vstack(X.mean_hindex_neighbours.values), ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVD5b0uqfkmB"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "feature_scaler = StandardScaler()\n",
        "feature_scaler.fit(X)\n",
        "X = feature_scaler.transform(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hQ3mF9EM0Pc"
      },
      "source": [
        "A_coo = A.tocoo()\n",
        "edge_index = np.vstack([A_coo.col, A_coo.row])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aznn7pDM5KX"
      },
      "source": [
        "## Graph Convolution Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR37OLXZM7u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffda7982-3e8a-4f9b-a90e-067a753ad6a1"
      },
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "if NEW_SESSION:\n",
        "  def format_pytorch_version(version):\n",
        "    return version.split('+')[0]\n",
        "\n",
        "  TORCH_version = torch.__version__\n",
        "  TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "  def format_cuda_version(version):\n",
        "    return 'cu' + version.replace('.', '')\n",
        "\n",
        "  CUDA_version = torch.version.cuda\n",
        "  CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "  !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html --quiet\n",
        "  !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html --quiet\n",
        "  !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html --quiet\n",
        "  !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html --quiet\n",
        "  !pip install torch-geometric --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 268 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 747 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 31.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAUoDrrSERmf"
      },
      "source": [
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.loader import DataLoader, RandomNodeSampler, NeighborLoader, ShaDowKHopSampler\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "from torch_geometric.data import Data\n",
        "import torch_geometric as tg\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNyPSo1263j"
      },
      "source": [
        "# Let's define our masks\n",
        "\n",
        "val1_idx, val2_idx = train_test_split(val1_idx, test_size=.5)\n",
        "\n",
        "mask_shape = (X.shape[0],)\n",
        "\n",
        "train_mask = torch.zeros(mask_shape, dtype=torch.bool)\n",
        "train_mask[train_idx] = True\n",
        "\n",
        "val1_mask = torch.zeros(mask_shape, dtype=torch.bool)\n",
        "val1_mask[val1_idx] = True\n",
        "\n",
        "val2_mask = torch.zeros(mask_shape, dtype=torch.bool)\n",
        "val2_mask[val2_idx] = True\n",
        "\n",
        "test_mask = torch.zeros(mask_shape, dtype=torch.bool)\n",
        "test_mask[test_idx] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjPeVV-GOg2_"
      },
      "source": [
        "n_features = X.shape[1]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIHTSF1Xqm1d"
      },
      "source": [
        "features = torch.tensor(X)\n",
        "targets = torch.tensor(y).unsqueeze(-1)\n",
        "edge_idx = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "data = Data(x=features, edge_index=edge_idx, y=targets)\n",
        "node_idx = torch.linspace(0, data.num_nodes-1, steps=data.num_nodes, dtype=torch.int64)\n",
        "\n",
        "# Let's define the masks\n",
        "data.train_mask = train_mask\n",
        "data.val1_mask = val1_mask\n",
        "data.val2_mask = val2_mask\n",
        "data.test_mask = test_mask\n",
        "data.node_index = node_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO9tRiRY7itf",
        "outputId": "d257ded0-4a29-4ef3-e8a3-cf7ab3832241"
      },
      "source": [
        "from torch_geometric.nn import GCNConv, SAGEConv, TransformerConv, GATConv, Linear, GCN2Conv\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Linear\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResGCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(ResGCN, self).__init__()\n",
        "\n",
        "        self.lin1 = Linear(data.num_node_features, hidden_channels)\n",
        "\n",
        "        #self.embedding = nn.Embedding(num_embeddings=data.num_nodes, embedding_dim=data.num_node_features)\n",
        "        self.conv1 = GCN2Conv(hidden_channels, alpha=.5)\n",
        "        self.conv2 = GCN2Conv(hidden_channels, alpha=.5)\n",
        "        self.conv3 = GCN2Conv(hidden_channels, alpha=.5)\n",
        "        self.conv4 = GCN2Conv(hidden_channels, alpha=.5)\n",
        "        self.lin_out = Linear(hidden_channels, 1)\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(p=.5)\n",
        "\n",
        "    def forward(self, x_0, edge_index, node_index):\n",
        "        #embs = self.embedding(node_index)\n",
        "        x_0 = x_0 #+ torch.nn.functional.normalize(embs, dim=1, p=2)\n",
        "\n",
        "        x_0 = self.lin1(x_0)\n",
        "        x_1 = self.act(x_0)\n",
        "        x_1 = self.dropout(x_1)\n",
        "\n",
        "        x_2 = self.conv1(x=x_1, x_0=x_0, edge_index=edge_index) \n",
        "        x_2 = self.act(x_2)\n",
        "        x_2 = self.dropout(x_2)\n",
        "\n",
        "        x_3 = self.conv2(x=x_2, x_0=x_1, edge_index=edge_index) \n",
        "        x_3 = self.act(x_3)\n",
        "        x_3 = self.dropout(x_3)\n",
        "\n",
        "        x_4 = self.conv3(x=x_3, x_0=x_1, edge_index=edge_index) \n",
        "        x_4 = self.act(x_4)\n",
        "        x_4 = self.dropout(x_4)\n",
        "\n",
        "        x_5 = self.lin_out(x_4)\n",
        "        \n",
        "        return x_5\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.lin1 = Linear(data.num_node_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin3 = Linear(hidden_channels, 1)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, x_0, edge_index, node_index):\n",
        "        x = self.lin1(x_0)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        skip = x\n",
        "\n",
        "        x = self.lin2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x + self.lin3(skip)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SAGENet(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(SAGENet, self).__init__()\n",
        "        #self.bn1 = nn.BatchNorm1d(data.num_node_features)\n",
        "        #self.embedding = nn.Embedding(num_embeddings=data.num_nodes, embedding_dim=data.num_node_features)\n",
        "        self.conv1 = SAGEConv(data.num_node_features, hidden_channels)\n",
        "        #self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, 1)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=.5)\n",
        "\n",
        "    def forward(self, x_0, edge_index, node_index):\n",
        "\n",
        "        x = self.conv1(x_0, edge_index)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = SAGENet(hidden_channels=250).to(device).double()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SAGENet(\n",
              "  (conv1): SAGEConv(721, 250)\n",
              "  (conv2): SAGEConv(250, 1)\n",
              "  (act): LeakyReLU(negative_slope=0.01)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CuOxa3TLfbJX",
        "outputId": "0f939912-bded-4b71-89d4-dd5391aa6e83"
      },
      "source": [
        "epochs = 1000\n",
        "\n",
        "criterion = nn.HuberLoss(delta=200) # The loss to optimize\n",
        "criterion_viz = nn.MSELoss() # The loss to print\n",
        "\n",
        "lowest_error = 1e9\n",
        "best_epoch = 0\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.4, verbose=True, patience=15)\n",
        "\n",
        "loader = RandomNodeSampler(data=data, num_parts=4, shuffle=True)\n",
        "#loader = ShaDowKHopSampler(data=data, depth=3, num_neighbors=5,)\n",
        "#loader = NeighborLoader(data=data, num_neighbors=[5], shuffle=True)\n",
        "\n",
        "def mean(ls): \n",
        "  return sum(ls)/len(ls)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  losses = {'train': [], 'val': []}\n",
        "  for phase in ['train', 'val',]:\n",
        "\n",
        "    if phase == 'train':\n",
        "      model.train()\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    for batch in loader:\n",
        "      input = batch.x\n",
        "\n",
        "      if phase == 'train':\n",
        "        optimizer.zero_grad()\n",
        "        batch.x[:, -1] = batch.x[:, -1] + (torch.randn(batch.x[:, -1].shape[0]) * noise * .0)\n",
        "\n",
        "\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      mask = batch.train_mask if phase == 'train' else batch.val1_mask\n",
        "\n",
        "      target = batch.y[mask]\n",
        "      out = model(batch.x, batch.edge_index, batch.node_index)[mask]\n",
        "\n",
        "      clamped_out = torch.clamp(out, min=1,)\n",
        "\n",
        "      loss = criterion(torch.clamp(out, max=35), target)\n",
        "      viz_loss = clamped_out #(torch.exp(out) - 1) * 5\n",
        "      viz_target = target #(torch.exp(target) - 1) * 5\n",
        "\n",
        "      with torch.no_grad():\n",
        "        viz_loss = criterion_viz(viz_loss, viz_target)\n",
        "        losses[phase].append(viz_loss.detach().item())\n",
        "\n",
        "      if phase == 'train':\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if phase == 'val':\n",
        "      epoch_val_loss = mean(losses['val'])\n",
        "      lr_scheduler.step(epoch_val_loss)\n",
        "      torch.save(model, f'models/epoch_{epoch}_model.pt')\n",
        "      \n",
        "      if lowest_error > epoch_val_loss:\n",
        "        lowest_error = epoch_val_loss\n",
        "        torch.save(model, 'models/best_model.pt')\n",
        "\n",
        "        best_epoch = epoch\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "  print(f\"[Epoch #{epoch}] Train Loss: {mean(losses['train'])} Val Loss: {mean(losses['val'])}\")\n",
        "\n",
        "print(f\"Loading model with val loss {lowest_error} from epoch {best_epoch}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Epoch #0] Train Loss: 206.98584438709884 Val Loss: 162.33809848156505\n",
            "\n",
            "[Epoch #1] Train Loss: 143.52745434266774 Val Loss: 141.731916223423\n",
            "[Epoch #2] Train Loss: 142.11522922390103 Val Loss: 144.53729449481835\n",
            "\n",
            "[Epoch #3] Train Loss: 138.24309536423925 Val Loss: 127.32363842562619\n",
            "\n",
            "[Epoch #4] Train Loss: 115.83732891668456 Val Loss: 106.04190170097885\n",
            "\n",
            "[Epoch #5] Train Loss: 99.90463380761308 Val Loss: 96.91323399915265\n",
            "\n",
            "[Epoch #6] Train Loss: 92.6065654572953 Val Loss: 91.8099298142849\n",
            "\n",
            "[Epoch #7] Train Loss: 88.60623486694257 Val Loss: 87.99141031195728\n",
            "\n",
            "[Epoch #8] Train Loss: 84.07702096128787 Val Loss: 83.8461674832817\n",
            "\n",
            "[Epoch #9] Train Loss: 81.03734857090159 Val Loss: 80.78936644027236\n",
            "\n",
            "[Epoch #10] Train Loss: 77.53452593133954 Val Loss: 77.55461696501521\n",
            "\n",
            "[Epoch #11] Train Loss: 74.95321210339989 Val Loss: 75.51186534312188\n",
            "\n",
            "[Epoch #12] Train Loss: 72.77952435640744 Val Loss: 73.3261804973913\n",
            "\n",
            "[Epoch #13] Train Loss: 70.3948611454488 Val Loss: 70.94786723322963\n",
            "\n",
            "[Epoch #14] Train Loss: 68.64649477127902 Val Loss: 69.28909852049676\n",
            "\n",
            "[Epoch #15] Train Loss: 66.93628022377034 Val Loss: 67.12511600723963\n",
            "\n",
            "[Epoch #16] Train Loss: 65.3380016034855 Val Loss: 65.83961160128754\n",
            "\n",
            "[Epoch #17] Train Loss: 63.48962002084825 Val Loss: 64.36032910057042\n",
            "\n",
            "[Epoch #18] Train Loss: 62.361531075535396 Val Loss: 62.849207138394505\n",
            "\n",
            "[Epoch #19] Train Loss: 60.92867124515668 Val Loss: 62.29985692388711\n",
            "\n",
            "[Epoch #20] Train Loss: 59.699727704751204 Val Loss: 60.82009531891943\n",
            "\n",
            "[Epoch #21] Train Loss: 59.16653730804752 Val Loss: 60.192383113669415\n",
            "\n",
            "[Epoch #22] Train Loss: 57.962008649728105 Val Loss: 59.60599984953275\n",
            "\n",
            "[Epoch #23] Train Loss: 57.43651547455237 Val Loss: 58.86039021850026\n",
            "\n",
            "[Epoch #24] Train Loss: 56.57542067657361 Val Loss: 58.45503190679777\n",
            "\n",
            "[Epoch #25] Train Loss: 56.019482556628105 Val Loss: 56.98080415973696\n",
            "[Epoch #26] Train Loss: 55.19037686552962 Val Loss: 57.07558282448952\n",
            "\n",
            "[Epoch #27] Train Loss: 54.87494028538669 Val Loss: 56.35441553803201\n",
            "\n",
            "[Epoch #28] Train Loss: 53.949317901902795 Val Loss: 55.12144764555143\n",
            "\n",
            "[Epoch #29] Train Loss: 53.299852853768726 Val Loss: 54.87365458818751\n",
            "\n",
            "[Epoch #30] Train Loss: 53.15346014033565 Val Loss: 54.519103451175255\n",
            "\n",
            "[Epoch #31] Train Loss: 52.57847386069929 Val Loss: 54.268049087922996\n",
            "\n",
            "[Epoch #32] Train Loss: 51.83954186959011 Val Loss: 53.44978263565259\n",
            "\n",
            "[Epoch #33] Train Loss: 51.77763192488581 Val Loss: 52.85409349965372\n",
            "\n",
            "[Epoch #34] Train Loss: 51.72222652283047 Val Loss: 52.660451185769574\n",
            "\n",
            "[Epoch #35] Train Loss: 50.80379126876369 Val Loss: 52.55642578712796\n",
            "\n",
            "[Epoch #36] Train Loss: 51.0371215336094 Val Loss: 52.35238965219149\n",
            "\n",
            "[Epoch #37] Train Loss: 50.022895871724074 Val Loss: 51.93552823940975\n",
            "[Epoch #38] Train Loss: 50.31929531589901 Val Loss: 52.039127538880635\n",
            "\n",
            "[Epoch #39] Train Loss: 50.05338055031022 Val Loss: 51.47056555736386\n",
            "[Epoch #40] Train Loss: 49.47607075470277 Val Loss: 52.25667846839587\n",
            "\n",
            "[Epoch #41] Train Loss: 49.52379955679332 Val Loss: 51.1960200271475\n",
            "[Epoch #42] Train Loss: 49.024326945638705 Val Loss: 51.32897731860413\n",
            "\n",
            "[Epoch #43] Train Loss: 49.06844328455046 Val Loss: 50.42676191347734\n",
            "[Epoch #44] Train Loss: 48.34558122790582 Val Loss: 50.964962177354934\n",
            "\n",
            "[Epoch #45] Train Loss: 48.26066713129511 Val Loss: 50.16572808842203\n",
            "\n",
            "[Epoch #46] Train Loss: 48.49166303880868 Val Loss: 50.03015353213049\n",
            "[Epoch #47] Train Loss: 47.94744842373662 Val Loss: 50.24372993230129\n",
            "[Epoch #48] Train Loss: 47.90000960312393 Val Loss: 50.045739164071186\n",
            "\n",
            "[Epoch #49] Train Loss: 47.62692796852504 Val Loss: 49.791810022483716\n",
            "\n",
            "[Epoch #50] Train Loss: 47.63129506932697 Val Loss: 49.40866405522503\n",
            "\n",
            "[Epoch #51] Train Loss: 47.383666318339706 Val Loss: 49.33228862103361\n",
            "[Epoch #52] Train Loss: 46.74533941650928 Val Loss: 49.90853460635279\n",
            "[Epoch #53] Train Loss: 46.94592666330768 Val Loss: 49.748491716863825\n",
            "[Epoch #54] Train Loss: 46.751162223479604 Val Loss: 49.409383379777935\n",
            "\n",
            "[Epoch #55] Train Loss: 47.05009385137446 Val Loss: 48.7846135159884\n",
            "[Epoch #56] Train Loss: 46.89663961168287 Val Loss: 49.53692132162885\n",
            "[Epoch #57] Train Loss: 46.99237361952316 Val Loss: 49.25407514557411\n",
            "\n",
            "[Epoch #58] Train Loss: 46.6731686938618 Val Loss: 48.41643761747839\n",
            "[Epoch #59] Train Loss: 46.13771983061912 Val Loss: 48.553506696549036\n",
            "\n",
            "[Epoch #60] Train Loss: 46.177798256061784 Val Loss: 47.97182082097915\n",
            "[Epoch #61] Train Loss: 45.87729985330852 Val Loss: 48.64800399904627\n",
            "[Epoch #62] Train Loss: 45.747734699943415 Val Loss: 49.00906739861797\n",
            "\n",
            "[Epoch #63] Train Loss: 45.61649767765645 Val Loss: 47.8563131798518\n",
            "[Epoch #64] Train Loss: 45.927743884079256 Val Loss: 48.396610688395334\n",
            "\n",
            "[Epoch #65] Train Loss: 45.86270661311889 Val Loss: 47.781083030316225\n",
            "[Epoch #66] Train Loss: 45.40174328381317 Val Loss: 47.97015867103341\n",
            "\n",
            "[Epoch #67] Train Loss: 45.47668280718874 Val Loss: 47.6722306501254\n",
            "\n",
            "[Epoch #68] Train Loss: 45.19907082071531 Val Loss: 47.564087074792596\n",
            "[Epoch #69] Train Loss: 45.10126119532037 Val Loss: 47.60132739014585\n",
            "[Epoch #70] Train Loss: 45.00635471872107 Val Loss: 48.15541427156422\n",
            "\n",
            "[Epoch #71] Train Loss: 45.052354499271395 Val Loss: 46.78842990683811\n",
            "[Epoch #72] Train Loss: 44.93291220020333 Val Loss: 48.32141118029079\n",
            "[Epoch #73] Train Loss: 44.89011796886354 Val Loss: 47.796294063138966\n",
            "[Epoch #74] Train Loss: 44.62481328469966 Val Loss: 48.379857939406314\n",
            "[Epoch #75] Train Loss: 44.89407659275139 Val Loss: 47.17790447500975\n",
            "[Epoch #76] Train Loss: 44.29267182530359 Val Loss: 47.933615876668135\n",
            "[Epoch #77] Train Loss: 44.58728690323784 Val Loss: 48.18451645460842\n",
            "[Epoch #78] Train Loss: 43.909840843794356 Val Loss: 47.01036882556733\n",
            "[Epoch #79] Train Loss: 44.165009051238954 Val Loss: 47.34471692596362\n",
            "[Epoch #80] Train Loss: 44.27282012730335 Val Loss: 47.649035004930724\n",
            "\n",
            "[Epoch #81] Train Loss: 44.13976763378529 Val Loss: 46.421604039831806\n",
            "[Epoch #82] Train Loss: 43.9767578984605 Val Loss: 46.6561335719379\n",
            "[Epoch #83] Train Loss: 44.08995837879489 Val Loss: 46.82505181195854\n",
            "[Epoch #84] Train Loss: 43.78073389612922 Val Loss: 46.75158354224047\n",
            "[Epoch #85] Train Loss: 43.685678385923815 Val Loss: 46.772882461763956\n",
            "[Epoch #86] Train Loss: 43.81461346383095 Val Loss: 46.66126248728193\n",
            "\n",
            "[Epoch #87] Train Loss: 43.66909978128827 Val Loss: 46.39620708666196\n",
            "\n",
            "[Epoch #88] Train Loss: 43.49338525092771 Val Loss: 45.901087393890414\n",
            "[Epoch #89] Train Loss: 43.6235410687775 Val Loss: 47.44559780174573\n",
            "[Epoch #90] Train Loss: 43.387481382172915 Val Loss: 46.93953049500192\n",
            "[Epoch #91] Train Loss: 43.47007668088425 Val Loss: 46.470321552215374\n",
            "[Epoch #92] Train Loss: 43.09418589116457 Val Loss: 46.28215793291574\n",
            "[Epoch #93] Train Loss: 43.124612090745444 Val Loss: 45.92145453881245\n",
            "[Epoch #94] Train Loss: 43.19465592262163 Val Loss: 46.7680392150702\n",
            "\n",
            "[Epoch #95] Train Loss: 42.76976459656178 Val Loss: 45.41881348333575\n",
            "[Epoch #96] Train Loss: 42.67281860037093 Val Loss: 46.88290953295342\n",
            "\n",
            "[Epoch #97] Train Loss: 42.82338236304879 Val Loss: 45.03944151984569\n",
            "\n",
            "[Epoch #98] Train Loss: 42.78726705405592 Val Loss: 44.95935137965711\n",
            "[Epoch #99] Train Loss: 42.73520809276513 Val Loss: 46.14174785385475\n",
            "[Epoch #100] Train Loss: 42.71812877838341 Val Loss: 45.157664876915\n",
            "[Epoch #101] Train Loss: 42.345109391945975 Val Loss: 45.60424094470064\n",
            "[Epoch #102] Train Loss: 42.699033394205 Val Loss: 46.43301763768207\n",
            "[Epoch #103] Train Loss: 42.175047652705246 Val Loss: 45.900692112106036\n",
            "[Epoch #104] Train Loss: 42.417489937364294 Val Loss: 45.65569009011064\n",
            "[Epoch #105] Train Loss: 42.406836904608504 Val Loss: 46.06865211790684\n",
            "[Epoch #106] Train Loss: 42.441734882002436 Val Loss: 45.75922557326045\n",
            "[Epoch #107] Train Loss: 41.868562083550415 Val Loss: 45.68203349770674\n",
            "[Epoch #108] Train Loss: 42.06933726527336 Val Loss: 45.551420387720924\n",
            "[Epoch #109] Train Loss: 41.88208050459621 Val Loss: 45.62183339546664\n",
            "[Epoch #110] Train Loss: 41.99859258623055 Val Loss: 45.37939623827584\n",
            "[Epoch #111] Train Loss: 41.55968368148307 Val Loss: 45.56179409534487\n",
            "[Epoch #112] Train Loss: 42.053584250711445 Val Loss: 45.48493081820404\n",
            "[Epoch #113] Train Loss: 41.90249033814325 Val Loss: 45.688645750625554\n",
            "Epoch   115: reducing learning rate of group 0 to 8.0000e-04.\n",
            "[Epoch #114] Train Loss: 41.49198450258619 Val Loss: 45.30315434816929\n",
            "\n",
            "[Epoch #115] Train Loss: 42.026178235957474 Val Loss: 44.90838428185457\n",
            "[Epoch #116] Train Loss: 41.50992171413279 Val Loss: 45.557611126814244\n",
            "[Epoch #117] Train Loss: 41.351135001869935 Val Loss: 45.634122768062376\n",
            "\n",
            "[Epoch #118] Train Loss: 41.45537635828255 Val Loss: 44.724886074813654\n",
            "[Epoch #119] Train Loss: 41.48653156548885 Val Loss: 44.852413832796536\n",
            "[Epoch #120] Train Loss: 41.30622244108421 Val Loss: 45.38533803575316\n",
            "[Epoch #121] Train Loss: 41.048617751485516 Val Loss: 45.1427181538265\n",
            "[Epoch #122] Train Loss: 41.5168175825787 Val Loss: 44.800155513645144\n",
            "[Epoch #123] Train Loss: 41.00188378817327 Val Loss: 45.22993797748805\n",
            "[Epoch #124] Train Loss: 41.47891638898733 Val Loss: 44.87177425611656\n",
            "[Epoch #125] Train Loss: 41.15946695672836 Val Loss: 45.0566899675982\n",
            "[Epoch #126] Train Loss: 40.942301964580395 Val Loss: 44.94022184760924\n",
            "[Epoch #127] Train Loss: 40.86523513372387 Val Loss: 45.39883263629884\n",
            "[Epoch #128] Train Loss: 40.68049891294296 Val Loss: 45.26782139020973\n",
            "[Epoch #129] Train Loss: 41.12578734117546 Val Loss: 45.41277017770294\n",
            "[Epoch #130] Train Loss: 40.79652371767571 Val Loss: 45.08110954322248\n",
            "\n",
            "[Epoch #131] Train Loss: 41.09165815420029 Val Loss: 44.227009286399664\n",
            "[Epoch #132] Train Loss: 40.8179849279386 Val Loss: 44.965389036935164\n",
            "[Epoch #133] Train Loss: 40.79672697786898 Val Loss: 45.32016592033754\n",
            "[Epoch #134] Train Loss: 40.84184364732934 Val Loss: 44.328058774513245\n",
            "[Epoch #135] Train Loss: 40.81652401562978 Val Loss: 45.14754878973867\n",
            "\n",
            "[Epoch #136] Train Loss: 41.09261749866439 Val Loss: 44.00972348934342\n",
            "[Epoch #137] Train Loss: 40.51271519314861 Val Loss: 45.321279684595\n",
            "[Epoch #138] Train Loss: 40.89669846983786 Val Loss: 44.708924246334064\n",
            "[Epoch #139] Train Loss: 40.735604039238275 Val Loss: 44.68104000051028\n",
            "[Epoch #140] Train Loss: 40.633909072238346 Val Loss: 44.811666417314626\n",
            "[Epoch #141] Train Loss: 40.79857898436808 Val Loss: 44.79963822454235\n",
            "[Epoch #142] Train Loss: 40.94569301999385 Val Loss: 45.894165381655114\n",
            "[Epoch #143] Train Loss: 40.522613010830156 Val Loss: 44.71603020738059\n",
            "[Epoch #144] Train Loss: 40.603316085981405 Val Loss: 44.56229938556834\n",
            "[Epoch #145] Train Loss: 40.65785516867784 Val Loss: 45.47844964584116\n",
            "[Epoch #146] Train Loss: 40.21687673829628 Val Loss: 44.09597961191527\n",
            "[Epoch #147] Train Loss: 40.70830258010332 Val Loss: 44.82663188377185\n",
            "[Epoch #148] Train Loss: 40.15916917182413 Val Loss: 44.878886259764435\n",
            "[Epoch #149] Train Loss: 40.33651657748617 Val Loss: 44.942147315883346\n",
            "[Epoch #150] Train Loss: 39.811285114771934 Val Loss: 45.02799201694496\n",
            "\n",
            "[Epoch #151] Train Loss: 39.962291749962645 Val Loss: 43.72914032931878\n",
            "\n",
            "[Epoch #152] Train Loss: 40.134517066977565 Val Loss: 43.366582389451025\n",
            "[Epoch #153] Train Loss: 40.31399579138963 Val Loss: 45.02113485547659\n",
            "[Epoch #154] Train Loss: 40.444289779638964 Val Loss: 44.68787303591701\n",
            "[Epoch #155] Train Loss: 40.275744868054744 Val Loss: 45.30119416782339\n",
            "[Epoch #156] Train Loss: 40.30430095637267 Val Loss: 44.32901888580792\n",
            "[Epoch #157] Train Loss: 40.17992770551151 Val Loss: 44.66840015050481\n",
            "[Epoch #158] Train Loss: 40.02483661448777 Val Loss: 43.690479378617916\n",
            "[Epoch #159] Train Loss: 40.241032115095535 Val Loss: 43.76023811976033\n",
            "[Epoch #160] Train Loss: 39.882850048790985 Val Loss: 44.25932510119011\n",
            "[Epoch #161] Train Loss: 40.14651524711528 Val Loss: 43.99250430799774\n",
            "[Epoch #162] Train Loss: 40.142248445772594 Val Loss: 44.46104155113185\n",
            "[Epoch #163] Train Loss: 40.114379797279284 Val Loss: 44.26467633190575\n",
            "[Epoch #164] Train Loss: 40.16861744676755 Val Loss: 43.85664583827005\n",
            "[Epoch #165] Train Loss: 39.74131701205805 Val Loss: 44.477962068802555\n",
            "[Epoch #166] Train Loss: 40.106788422764204 Val Loss: 44.615950447122245\n",
            "[Epoch #167] Train Loss: 39.760884528873426 Val Loss: 44.678091911586165\n",
            "Epoch   169: reducing learning rate of group 0 to 3.2000e-04.\n",
            "[Epoch #168] Train Loss: 40.02994739668965 Val Loss: 44.565521457046295\n",
            "[Epoch #169] Train Loss: 39.81880563863518 Val Loss: 43.76501445305509\n",
            "[Epoch #170] Train Loss: 39.87220653861451 Val Loss: 43.89880522103567\n",
            "[Epoch #171] Train Loss: 39.78155942347289 Val Loss: 43.649033515627764\n",
            "[Epoch #172] Train Loss: 39.73365941959989 Val Loss: 44.62241878289122\n",
            "[Epoch #173] Train Loss: 39.62088236579814 Val Loss: 43.92309240219281\n",
            "[Epoch #174] Train Loss: 39.40844093859845 Val Loss: 44.04172199340192\n",
            "[Epoch #175] Train Loss: 39.583063112625254 Val Loss: 44.120205099300755\n",
            "[Epoch #176] Train Loss: 39.74442998986891 Val Loss: 44.2058690589234\n",
            "\n",
            "[Epoch #177] Train Loss: 39.541203538922886 Val Loss: 43.01818981040711\n",
            "[Epoch #178] Train Loss: 39.586128746403716 Val Loss: 44.292984072478895\n",
            "[Epoch #179] Train Loss: 39.408524238596044 Val Loss: 44.60123135908198\n",
            "[Epoch #180] Train Loss: 39.442033793063885 Val Loss: 44.40867947595727\n",
            "[Epoch #181] Train Loss: 39.4771271547924 Val Loss: 43.494251446389896\n",
            "[Epoch #182] Train Loss: 39.526651878930025 Val Loss: 43.964325414353375\n",
            "[Epoch #183] Train Loss: 39.60431462665754 Val Loss: 43.801410474831165\n",
            "[Epoch #184] Train Loss: 39.80387189891772 Val Loss: 44.42740110281386\n",
            "[Epoch #185] Train Loss: 39.68750080587534 Val Loss: 44.306083509949715\n",
            "[Epoch #186] Train Loss: 39.30597744154109 Val Loss: 43.43635923784734\n",
            "[Epoch #187] Train Loss: 39.58191422810812 Val Loss: 44.24715896373378\n",
            "[Epoch #188] Train Loss: 39.68311670440626 Val Loss: 43.28606512796083\n",
            "[Epoch #189] Train Loss: 39.64780915974427 Val Loss: 44.21917350584777\n",
            "[Epoch #190] Train Loss: 39.44866243716963 Val Loss: 43.19609436971035\n",
            "[Epoch #191] Train Loss: 39.61638401550038 Val Loss: 43.79259557521486\n",
            "[Epoch #192] Train Loss: 39.4610075133012 Val Loss: 43.86859495105084\n",
            "Epoch   194: reducing learning rate of group 0 to 1.2800e-04.\n",
            "[Epoch #193] Train Loss: 39.31022839623077 Val Loss: 43.9103189178259\n",
            "[Epoch #194] Train Loss: 39.59441744596634 Val Loss: 44.806482704311776\n",
            "[Epoch #195] Train Loss: 39.520270365989965 Val Loss: 43.925374482942416\n",
            "[Epoch #196] Train Loss: 39.79017795940211 Val Loss: 43.86404837150179\n",
            "[Epoch #197] Train Loss: 39.12686012426056 Val Loss: 44.1148706198242\n",
            "[Epoch #198] Train Loss: 39.57353005717216 Val Loss: 43.62932555838592\n",
            "[Epoch #199] Train Loss: 39.695183190667095 Val Loss: 44.27110831585777\n",
            "[Epoch #200] Train Loss: 39.34173241690142 Val Loss: 44.07793065652539\n",
            "[Epoch #201] Train Loss: 39.3329654401697 Val Loss: 44.42613567461409\n",
            "[Epoch #202] Train Loss: 39.42998403156859 Val Loss: 43.68228876329183\n",
            "[Epoch #203] Train Loss: 39.51624625241788 Val Loss: 43.599428327051896\n",
            "[Epoch #204] Train Loss: 39.09638369515979 Val Loss: 44.5164859956614\n",
            "[Epoch #205] Train Loss: 39.41053683785559 Val Loss: 44.47763856372707\n",
            "[Epoch #206] Train Loss: 39.608358079551955 Val Loss: 43.81853932339179\n",
            "[Epoch #207] Train Loss: 39.1144540030403 Val Loss: 44.07904665030309\n",
            "[Epoch #208] Train Loss: 39.368805244425815 Val Loss: 44.27267487728416\n",
            "Epoch   210: reducing learning rate of group 0 to 5.1200e-05.\n",
            "[Epoch #209] Train Loss: 39.365245411740204 Val Loss: 43.36056768027588\n",
            "[Epoch #210] Train Loss: 39.33374626302773 Val Loss: 43.64977007495616\n",
            "[Epoch #211] Train Loss: 39.368633414284666 Val Loss: 43.79475470527396\n",
            "[Epoch #212] Train Loss: 39.21659543023269 Val Loss: 43.654071082358584\n",
            "[Epoch #213] Train Loss: 39.18334962161177 Val Loss: 44.392761082571425\n",
            "[Epoch #214] Train Loss: 39.23094808600912 Val Loss: 44.634336657059904\n",
            "[Epoch #215] Train Loss: 39.131816926172675 Val Loss: 44.87155932678793\n",
            "[Epoch #216] Train Loss: 39.3903970408738 Val Loss: 44.06704793035414\n",
            "[Epoch #217] Train Loss: 38.743821287045805 Val Loss: 43.746433420598144\n",
            "[Epoch #218] Train Loss: 39.20981108750148 Val Loss: 43.84263623284944\n",
            "[Epoch #219] Train Loss: 39.29153975591703 Val Loss: 44.10924395942665\n",
            "[Epoch #220] Train Loss: 39.23745011154911 Val Loss: 43.281861860921346\n",
            "[Epoch #221] Train Loss: 39.28652773230534 Val Loss: 43.6594039260067\n",
            "[Epoch #222] Train Loss: 39.458078909700056 Val Loss: 43.93880373917465\n",
            "[Epoch #223] Train Loss: 39.20438112196244 Val Loss: 43.9158529636244\n",
            "[Epoch #224] Train Loss: 39.27546277273246 Val Loss: 44.02821435447857\n",
            "Epoch   226: reducing learning rate of group 0 to 2.0480e-05.\n",
            "[Epoch #225] Train Loss: 39.43031589983403 Val Loss: 43.88635600533651\n",
            "[Epoch #226] Train Loss: 39.461088143073276 Val Loss: 44.536036736868255\n",
            "[Epoch #227] Train Loss: 39.355752674547105 Val Loss: 43.4275820272138\n",
            "[Epoch #228] Train Loss: 38.98798517064198 Val Loss: 44.98626062411279\n",
            "[Epoch #229] Train Loss: 39.28038469342213 Val Loss: 43.69376680669221\n",
            "[Epoch #230] Train Loss: 39.29130922712823 Val Loss: 43.55946585263144\n",
            "[Epoch #231] Train Loss: 38.99366655687665 Val Loss: 43.036253872174065\n",
            "[Epoch #232] Train Loss: 39.32461963787937 Val Loss: 44.430277668760105\n",
            "[Epoch #233] Train Loss: 39.010325640080055 Val Loss: 44.57553485176612\n",
            "[Epoch #234] Train Loss: 39.24233864898366 Val Loss: 43.759712803482955\n",
            "[Epoch #235] Train Loss: 39.01267505867072 Val Loss: 44.3355028237829\n",
            "[Epoch #236] Train Loss: 38.92759067308547 Val Loss: 44.91593530084947\n",
            "[Epoch #237] Train Loss: 39.27390829007736 Val Loss: 43.88709188675246\n",
            "[Epoch #238] Train Loss: 39.06354593634933 Val Loss: 45.03506656762271\n",
            "[Epoch #239] Train Loss: 39.2100886923164 Val Loss: 44.42612615597376\n",
            "[Epoch #240] Train Loss: 39.178668780082894 Val Loss: 43.47364826368177\n",
            "Epoch   242: reducing learning rate of group 0 to 8.1920e-06.\n",
            "[Epoch #241] Train Loss: 39.35336530411523 Val Loss: 43.613872300545616\n",
            "[Epoch #242] Train Loss: 39.57284810435319 Val Loss: 44.11318040430467\n",
            "[Epoch #243] Train Loss: 39.066121211846884 Val Loss: 44.33178232339692\n",
            "[Epoch #244] Train Loss: 38.739449994768506 Val Loss: 43.94692311946693\n",
            "[Epoch #245] Train Loss: 39.15723405720854 Val Loss: 43.81006134872478\n",
            "[Epoch #246] Train Loss: 39.0901558294799 Val Loss: 43.95006553834854\n",
            "[Epoch #247] Train Loss: 39.27393629183223 Val Loss: 43.89127330265025\n",
            "[Epoch #248] Train Loss: 38.95242958619094 Val Loss: 44.438185080073026\n",
            "[Epoch #249] Train Loss: 39.10960837636226 Val Loss: 43.887303851652916\n",
            "[Epoch #250] Train Loss: 39.38959414197875 Val Loss: 43.89102439194825\n",
            "[Epoch #251] Train Loss: 39.32181046915647 Val Loss: 43.98056154570758\n",
            "[Epoch #252] Train Loss: 39.49964288537592 Val Loss: 44.109040937679026\n",
            "[Epoch #253] Train Loss: 39.33428222453999 Val Loss: 43.9722262247776\n",
            "[Epoch #254] Train Loss: 39.27957487888723 Val Loss: 43.63020069333608\n",
            "[Epoch #255] Train Loss: 39.18974146473512 Val Loss: 43.96990739244434\n",
            "[Epoch #256] Train Loss: 39.18891654206653 Val Loss: 43.989339843580375\n",
            "Epoch   258: reducing learning rate of group 0 to 3.2768e-06.\n",
            "[Epoch #257] Train Loss: 39.04842040144249 Val Loss: 43.68900286949597\n",
            "[Epoch #258] Train Loss: 38.93982363840902 Val Loss: 43.219857351308505\n",
            "[Epoch #259] Train Loss: 39.405248536929754 Val Loss: 43.862356695940676\n",
            "[Epoch #260] Train Loss: 39.20683874848578 Val Loss: 43.82658776296648\n",
            "[Epoch #261] Train Loss: 39.411162297355595 Val Loss: 43.87005928748345\n",
            "[Epoch #262] Train Loss: 39.34351035869851 Val Loss: 44.483150095132956\n",
            "[Epoch #263] Train Loss: 39.01855702862539 Val Loss: 44.25075924189955\n",
            "[Epoch #264] Train Loss: 39.04787870391429 Val Loss: 44.82552944888389\n",
            "[Epoch #265] Train Loss: 39.67293384028538 Val Loss: 44.020066375747234\n",
            "[Epoch #266] Train Loss: 39.04776024885759 Val Loss: 43.803192204170095\n",
            "[Epoch #267] Train Loss: 39.31368605961814 Val Loss: 44.27114479047056\n",
            "[Epoch #268] Train Loss: 38.9607628118105 Val Loss: 44.19005389125351\n",
            "[Epoch #269] Train Loss: 39.07103955743912 Val Loss: 44.38264407778789\n",
            "[Epoch #270] Train Loss: 39.46308371116336 Val Loss: 43.44151645362082\n",
            "[Epoch #271] Train Loss: 39.189696586184546 Val Loss: 43.79988009812282\n",
            "[Epoch #272] Train Loss: 39.40017688472049 Val Loss: 43.8769041133453\n",
            "Epoch   274: reducing learning rate of group 0 to 1.3107e-06.\n",
            "[Epoch #273] Train Loss: 39.338205983301634 Val Loss: 44.44617564882039\n",
            "[Epoch #274] Train Loss: 39.06567325368955 Val Loss: 44.14651444161361\n",
            "[Epoch #275] Train Loss: 39.29394474834356 Val Loss: 44.05817258964542\n",
            "[Epoch #276] Train Loss: 39.388788322997556 Val Loss: 43.93434440150039\n",
            "[Epoch #277] Train Loss: 39.10052417922075 Val Loss: 44.0607972986104\n",
            "[Epoch #278] Train Loss: 38.9320789884254 Val Loss: 44.89201285479205\n",
            "[Epoch #279] Train Loss: 38.81035950971842 Val Loss: 43.59425369118357\n",
            "[Epoch #280] Train Loss: 39.35944908840815 Val Loss: 44.475052971922956\n",
            "[Epoch #281] Train Loss: 39.192540929838515 Val Loss: 44.55352804967571\n",
            "[Epoch #282] Train Loss: 39.1105145720759 Val Loss: 44.18651225367313\n",
            "[Epoch #283] Train Loss: 38.81598968108431 Val Loss: 44.085718189660255\n",
            "[Epoch #284] Train Loss: 39.109975964424976 Val Loss: 44.2803373941219\n",
            "[Epoch #285] Train Loss: 39.07956933077709 Val Loss: 44.46173451114196\n",
            "[Epoch #286] Train Loss: 39.49892820492342 Val Loss: 43.79715375895638\n",
            "[Epoch #287] Train Loss: 39.09757181793668 Val Loss: 43.626919794584765\n",
            "[Epoch #288] Train Loss: 39.342922624727954 Val Loss: 44.12314227243063\n",
            "Epoch   290: reducing learning rate of group 0 to 5.2429e-07.\n",
            "[Epoch #289] Train Loss: 39.58403177253378 Val Loss: 43.917772447008915\n",
            "[Epoch #290] Train Loss: 39.166985455858175 Val Loss: 44.702173801310515\n",
            "[Epoch #291] Train Loss: 39.254271697245194 Val Loss: 43.677489728240865\n",
            "[Epoch #292] Train Loss: 39.50351507199962 Val Loss: 43.43311622239162\n",
            "[Epoch #293] Train Loss: 39.36708349619121 Val Loss: 43.705028544291665\n",
            "[Epoch #294] Train Loss: 39.436842241869066 Val Loss: 43.26262346676369\n",
            "[Epoch #295] Train Loss: 39.405746624241544 Val Loss: 44.018305954864765\n",
            "[Epoch #296] Train Loss: 39.170213201757264 Val Loss: 43.838523379257786\n",
            "[Epoch #297] Train Loss: 39.039319700416826 Val Loss: 44.88756851260783\n",
            "[Epoch #298] Train Loss: 38.857775708551884 Val Loss: 43.36757596806203\n",
            "[Epoch #299] Train Loss: 39.389019511318104 Val Loss: 43.593347543285354\n",
            "[Epoch #300] Train Loss: 39.601964744536915 Val Loss: 43.40032274082851\n",
            "[Epoch #301] Train Loss: 39.29008567886204 Val Loss: 43.29498534684758\n",
            "[Epoch #302] Train Loss: 39.21345186803059 Val Loss: 44.49715376267082\n",
            "[Epoch #303] Train Loss: 39.06929997869024 Val Loss: 43.53875022865909\n",
            "[Epoch #304] Train Loss: 39.40856601840222 Val Loss: 44.24078132775693\n",
            "Epoch   306: reducing learning rate of group 0 to 2.0972e-07.\n",
            "[Epoch #305] Train Loss: 38.919246072497906 Val Loss: 44.171156150130315\n",
            "[Epoch #306] Train Loss: 39.256743307702806 Val Loss: 44.081462360678984\n",
            "[Epoch #307] Train Loss: 39.351376351054135 Val Loss: 43.25196813522997\n",
            "[Epoch #308] Train Loss: 38.872340451092064 Val Loss: 44.0719795998\n",
            "[Epoch #309] Train Loss: 39.02135591808821 Val Loss: 44.19917096369534\n",
            "[Epoch #310] Train Loss: 39.337373414349074 Val Loss: 43.53505235557928\n",
            "[Epoch #311] Train Loss: 39.37583059214012 Val Loss: 43.44387139010496\n",
            "[Epoch #312] Train Loss: 39.367533025000924 Val Loss: 44.01762848728737\n",
            "[Epoch #313] Train Loss: 39.56065953737933 Val Loss: 44.43314825110676\n",
            "[Epoch #314] Train Loss: 38.86489246800974 Val Loss: 43.81379064869384\n",
            "[Epoch #315] Train Loss: 39.37384827744893 Val Loss: 43.888852410993955\n",
            "[Epoch #316] Train Loss: 39.089839552506774 Val Loss: 44.19378658734441\n",
            "[Epoch #317] Train Loss: 38.971382909432855 Val Loss: 44.22778657283382\n",
            "[Epoch #318] Train Loss: 39.026484423322735 Val Loss: 43.66981256431044\n",
            "[Epoch #319] Train Loss: 39.143301491664026 Val Loss: 43.81545992251098\n",
            "[Epoch #320] Train Loss: 39.28259535967551 Val Loss: 43.159361119338165\n",
            "Epoch   322: reducing learning rate of group 0 to 8.3886e-08.\n",
            "[Epoch #321] Train Loss: 39.0174195512121 Val Loss: 44.291601813403744\n",
            "[Epoch #322] Train Loss: 39.05224571135371 Val Loss: 44.68314093467204\n",
            "[Epoch #323] Train Loss: 39.115975731451734 Val Loss: 43.8513513192626\n",
            "[Epoch #324] Train Loss: 39.33965754607569 Val Loss: 44.291200820694016\n",
            "[Epoch #325] Train Loss: 39.39376423911074 Val Loss: 44.04000357253363\n",
            "[Epoch #326] Train Loss: 39.44304558771201 Val Loss: 44.52885266250575\n",
            "[Epoch #327] Train Loss: 39.478345672575216 Val Loss: 43.05946720571384\n",
            "[Epoch #328] Train Loss: 39.206765575513614 Val Loss: 44.88879569685579\n",
            "[Epoch #329] Train Loss: 39.33680149962043 Val Loss: 43.81978089966866\n",
            "[Epoch #330] Train Loss: 39.31287578118451 Val Loss: 44.17990303319738\n",
            "[Epoch #331] Train Loss: 39.085618953757184 Val Loss: 43.910805750102625\n",
            "[Epoch #332] Train Loss: 39.171127695871775 Val Loss: 43.20287434344047\n",
            "[Epoch #333] Train Loss: 39.3120452749496 Val Loss: 44.66114027812362\n",
            "[Epoch #334] Train Loss: 39.21748052629536 Val Loss: 43.557354123082604\n",
            "[Epoch #335] Train Loss: 39.39944994489056 Val Loss: 44.539508068409965\n",
            "[Epoch #336] Train Loss: 39.02232379500137 Val Loss: 43.06519222767192\n",
            "Epoch   338: reducing learning rate of group 0 to 3.3554e-08.\n",
            "[Epoch #337] Train Loss: 39.30142788286611 Val Loss: 43.713800297921246\n",
            "[Epoch #338] Train Loss: 39.08840979348694 Val Loss: 43.4153185706623\n",
            "[Epoch #339] Train Loss: 39.1583230862824 Val Loss: 43.49374375397046\n",
            "[Epoch #340] Train Loss: 39.137414138623626 Val Loss: 44.59177372711723\n",
            "[Epoch #341] Train Loss: 39.51322735600511 Val Loss: 43.29404090050419\n",
            "[Epoch #342] Train Loss: 39.227868489450586 Val Loss: 43.43225635801681\n",
            "[Epoch #343] Train Loss: 39.17699886715839 Val Loss: 44.26548683405241\n",
            "[Epoch #344] Train Loss: 39.191256759098145 Val Loss: 44.10525186979283\n",
            "[Epoch #345] Train Loss: 39.06154352307941 Val Loss: 44.25203736250056\n",
            "[Epoch #346] Train Loss: 39.04314683630064 Val Loss: 43.62125967690754\n",
            "[Epoch #347] Train Loss: 39.27476812191484 Val Loss: 43.99170042088557\n",
            "[Epoch #348] Train Loss: 39.27281514655547 Val Loss: 43.74537682898123\n",
            "[Epoch #349] Train Loss: 39.38516099546562 Val Loss: 44.12107542107511\n",
            "[Epoch #350] Train Loss: 39.302424413985364 Val Loss: 43.768977835749844\n",
            "[Epoch #351] Train Loss: 38.97827971294893 Val Loss: 44.01996674020596\n",
            "[Epoch #352] Train Loss: 39.44250717922027 Val Loss: 43.28953147558481\n",
            "Epoch   354: reducing learning rate of group 0 to 1.3422e-08.\n",
            "[Epoch #353] Train Loss: 39.16808125335545 Val Loss: 43.31037875346952\n",
            "[Epoch #354] Train Loss: 39.14144885216547 Val Loss: 44.38696259000185\n",
            "[Epoch #355] Train Loss: 39.371232175606835 Val Loss: 44.54389831861314\n",
            "[Epoch #356] Train Loss: 39.33573265135103 Val Loss: 44.59693372007278\n",
            "[Epoch #357] Train Loss: 39.159721132484364 Val Loss: 43.17779372849548\n",
            "[Epoch #358] Train Loss: 39.08479510708856 Val Loss: 44.25875565408785\n",
            "[Epoch #359] Train Loss: 39.48859171749095 Val Loss: 43.84342865907928\n",
            "[Epoch #360] Train Loss: 39.16278294847573 Val Loss: 44.14112652051385\n",
            "[Epoch #361] Train Loss: 39.26346377014398 Val Loss: 44.35659637448113\n",
            "[Epoch #362] Train Loss: 39.09831940372803 Val Loss: 43.98848898642153\n",
            "[Epoch #363] Train Loss: 39.10901898303139 Val Loss: 43.58001219243799\n",
            "[Epoch #364] Train Loss: 39.37326791834997 Val Loss: 44.37449184214103\n",
            "[Epoch #365] Train Loss: 39.42999691295402 Val Loss: 44.323160411846594\n",
            "[Epoch #366] Train Loss: 39.45477912082225 Val Loss: 44.2242056114132\n",
            "[Epoch #367] Train Loss: 39.348411143542364 Val Loss: 45.069212042892\n",
            "[Epoch #368] Train Loss: 39.31802812395487 Val Loss: 44.31208772980561\n",
            "[Epoch #369] Train Loss: 39.093834965381944 Val Loss: 44.27646898410169\n",
            "[Epoch #370] Train Loss: 39.377690526390325 Val Loss: 43.82709179875933\n",
            "[Epoch #371] Train Loss: 39.28535284267352 Val Loss: 43.414170223092064\n",
            "[Epoch #372] Train Loss: 39.306383734425495 Val Loss: 44.45149083478502\n",
            "[Epoch #373] Train Loss: 39.19980241091962 Val Loss: 43.95830673566385\n",
            "[Epoch #374] Train Loss: 39.318837329088964 Val Loss: 44.44728640495678\n",
            "[Epoch #375] Train Loss: 39.342804805492996 Val Loss: 44.25776635307162\n",
            "[Epoch #376] Train Loss: 39.30674818200933 Val Loss: 43.25820135253751\n",
            "[Epoch #377] Train Loss: 39.17313338747239 Val Loss: 44.49033675301281\n",
            "[Epoch #378] Train Loss: 39.338315495078824 Val Loss: 43.87925441300763\n",
            "[Epoch #379] Train Loss: 39.30347730817205 Val Loss: 44.35429905932915\n",
            "[Epoch #380] Train Loss: 39.15020696285923 Val Loss: 44.201756970665\n",
            "[Epoch #381] Train Loss: 38.997384790598026 Val Loss: 43.73307063955845\n",
            "[Epoch #382] Train Loss: 39.12667301471272 Val Loss: 43.49881179439827\n",
            "[Epoch #383] Train Loss: 38.91330887304279 Val Loss: 44.189223887078086\n",
            "[Epoch #384] Train Loss: 39.370098253683786 Val Loss: 44.31437030861502\n",
            "[Epoch #385] Train Loss: 39.483348130970185 Val Loss: 43.574803438298446\n",
            "[Epoch #386] Train Loss: 39.26135928211981 Val Loss: 43.487128110479176\n",
            "[Epoch #387] Train Loss: 39.05213760930974 Val Loss: 43.984738361876495\n",
            "[Epoch #388] Train Loss: 39.265097091202804 Val Loss: 44.154946573643585\n",
            "[Epoch #389] Train Loss: 39.2074539750632 Val Loss: 43.76906244087181\n",
            "[Epoch #390] Train Loss: 39.70837310413014 Val Loss: 43.61571863908303\n",
            "[Epoch #391] Train Loss: 39.37555762348062 Val Loss: 43.9393071253898\n",
            "[Epoch #392] Train Loss: 38.988328436993115 Val Loss: 43.5110814835245\n",
            "[Epoch #393] Train Loss: 39.171994057924266 Val Loss: 44.508483216542984\n",
            "[Epoch #394] Train Loss: 39.36186433162947 Val Loss: 43.827036142795706\n",
            "[Epoch #395] Train Loss: 39.55301050220504 Val Loss: 44.097257094030226\n",
            "[Epoch #396] Train Loss: 39.21965196119664 Val Loss: 43.48437388840712\n",
            "[Epoch #397] Train Loss: 39.40530181274261 Val Loss: 44.74168910439981\n",
            "[Epoch #398] Train Loss: 39.22547040896589 Val Loss: 44.897236850414586\n",
            "[Epoch #399] Train Loss: 39.11516814986531 Val Loss: 43.878294528940415\n",
            "[Epoch #400] Train Loss: 39.54343347565842 Val Loss: 44.096072990721865\n",
            "[Epoch #401] Train Loss: 39.43788464298315 Val Loss: 44.20728359186318\n",
            "[Epoch #402] Train Loss: 39.274508165541164 Val Loss: 43.683152488030586\n",
            "[Epoch #403] Train Loss: 39.382250416806116 Val Loss: 44.843500507934294\n",
            "[Epoch #404] Train Loss: 39.10046959246574 Val Loss: 43.913202486823145\n",
            "[Epoch #405] Train Loss: 39.23640571627041 Val Loss: 44.17746258235354\n",
            "[Epoch #406] Train Loss: 39.41875785269467 Val Loss: 44.328911216472065\n",
            "[Epoch #407] Train Loss: 39.22834283421748 Val Loss: 43.91113131951634\n",
            "[Epoch #408] Train Loss: 39.44104884278234 Val Loss: 43.64006131310781\n",
            "[Epoch #409] Train Loss: 39.62142983038677 Val Loss: 45.061228916391826\n",
            "[Epoch #410] Train Loss: 39.32530004422751 Val Loss: 43.68074069344396\n",
            "[Epoch #411] Train Loss: 38.77073020669506 Val Loss: 43.64322791292308\n",
            "[Epoch #412] Train Loss: 38.764189226203484 Val Loss: 44.15172172331083\n",
            "[Epoch #413] Train Loss: 39.02449176775437 Val Loss: 43.317050119780276\n",
            "[Epoch #414] Train Loss: 39.2846333920225 Val Loss: 44.02753985440374\n",
            "[Epoch #415] Train Loss: 39.2058754916129 Val Loss: 43.67641693620943\n",
            "[Epoch #416] Train Loss: 38.73224545414625 Val Loss: 43.33735406049284\n",
            "[Epoch #417] Train Loss: 39.06894407745221 Val Loss: 44.093006098001574\n",
            "[Epoch #418] Train Loss: 39.17816468422884 Val Loss: 43.81245655412754\n",
            "[Epoch #419] Train Loss: 39.21181422524654 Val Loss: 43.748532915283704\n",
            "[Epoch #420] Train Loss: 39.37303695610909 Val Loss: 43.92707243282473\n",
            "[Epoch #421] Train Loss: 39.17691873743168 Val Loss: 43.914553388143624\n",
            "[Epoch #422] Train Loss: 39.22980593797064 Val Loss: 43.57574680880804\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-e423f423c277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/loader/random_node_sampler.py\u001b[0m in \u001b[0;36m__collate__\u001b[0;34m(self, node_idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SbWIbEjbbPP"
      },
      "source": [
        "model = torch.load(f'models/best_model.pt')\n",
        "model = torch.load(f'models/epoch_{163}_model.pt')\n",
        "\n",
        "!cp 'models/best_model.pt' '/content/drive/MyDrive/saved_models/best_model_hindex.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIwr6LydIo2m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "b8c3a047-b688-40e5-c451-3f4454f2d7c5"
      },
      "source": [
        "preds = []\n",
        "\n",
        "loader = RandomNodeSampler(data=data, num_parts=2, shuffle=False)\n",
        "with torch.no_grad():\n",
        "  model.eval().to(device)\n",
        "  for batch in loader:\n",
        "    batch = batch.to(device)\n",
        "    out = model(batch.x, batch.edge_index, batch.node_index).cpu()\n",
        "    preds.append((batch.node_index.cpu(), out))\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for i in range(len(preds)):\n",
        "  for idx, val in zip(preds[i][0], preds[i][1]):\n",
        "    pairs.append((idx.item(), val.item()))\n",
        "\n",
        "pairs = sorted(pairs, key=lambda x: x[0])\n",
        "\n",
        "pairs = pd.DataFrame(pairs, columns=['author', 'pred']).set_index('author')\n",
        "pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>author</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.385676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.374938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.859091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.621687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.269971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217796</th>\n",
              "      <td>18.744655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217797</th>\n",
              "      <td>9.944682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217798</th>\n",
              "      <td>3.736912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217799</th>\n",
              "      <td>1.207111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217800</th>\n",
              "      <td>6.856253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>217801 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             pred\n",
              "author           \n",
              "0        6.385676\n",
              "1        0.374938\n",
              "2       12.859091\n",
              "3        1.621687\n",
              "4        0.269971\n",
              "...           ...\n",
              "217796  18.744655\n",
              "217797   9.944682\n",
              "217798   3.736912\n",
              "217799   1.207111\n",
              "217800   6.856253\n",
              "\n",
              "[217801 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PeM0pWmANiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe7a698-3ee4-4181-841b-6245f649126e"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mask = train_mask.numpy()\n",
        "train.loc[node_idx[mask], \"pred\"] = np.clip(pairs.loc[mask, \"pred\"], 1, None)\n",
        "mean_squared_error(train.loc[node_idx[mask], \"pred\"], train.loc[node_idx[mask], \"hindex\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.13945334487358"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJV4MDOu3McT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77622f4a-7231-46fd-f52a-96afd1d7da0e"
      },
      "source": [
        "mask = val1_mask.numpy()\n",
        "train.loc[node_idx[mask], \"pred\"] = np.clip(pairs.loc[mask, \"pred\"], 1, None)\n",
        "mean_squared_error(train.loc[node_idx[mask], \"pred\"], train.loc[node_idx[mask], \"hindex\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42.13393948098483"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjuvAYCfTNT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e25aec-e00d-4b91-f4be-d504c4af681f"
      },
      "source": [
        "mask = val2_mask.numpy()\n",
        "train.loc[node_idx[mask], \"pred\"] = np.clip(pairs.loc[mask, \"pred\"], 1, None)\n",
        "mean_squared_error(train.loc[node_idx[mask], \"pred\"], train.loc[node_idx[mask], \"hindex\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.19747084973854"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4OXCp3mlQgU"
      },
      "source": [
        "mask = test_mask.numpy()\n",
        "test.loc[node_idx[mask], \"hindex\"] = np.clip(pairs.loc[mask, \"pred\"], 1, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-XTlYIF3rYr"
      },
      "source": [
        "test[[\"author\", \"hindex\"]].to_csv(\"submission_hindex2.csv\", index=False)\n",
        "!kaggle competitions submit -c inf554-2021 -f submission_hindex2.csv -m \"SageConv v9, Extended with Second Transformer and H-Index\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAHVW3nlZV4z"
      },
      "source": [
        "test[[\"author\", \"hindex\"]].to_csv(\"submission_res4.csv\", index=False)\n",
        "#!kaggle competitions submit -c inf554-2021 -f submission_res3.csv -m \"Residual GCN v3\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}